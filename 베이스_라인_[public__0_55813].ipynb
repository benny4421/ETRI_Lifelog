{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hgziZCG__AC3",
        "ybuDtSKSuivs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LGBM"
      ],
      "metadata": {
        "id": "88auPlbC9a3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
      ],
      "metadata": {
        "id": "dQPreH70lh4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h46RU-gNcEJj",
        "outputId": "14ee75f6-b149-4292-b950-82c95e5e75ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ast\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "eJ0BkxNkf01P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed ê³ ì •\n",
        "SD = 42\n",
        "random.seed(SD)\n",
        "np.random.seed(SD)\n",
        "os.environ['PYTHONHASHSEED'] = str(SD)"
      ],
      "metadata": {
        "id": "RuPZU7og9khw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "Kgs6iERh9CzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë°ì´í„° ë¡œë“œ"
      ],
      "metadata": {
        "id": "hgziZCG__AC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "data_dir = '/content/drive/MyDrive/ETRI_lifelog_dataset/ch2025_data_items'\n",
        "\n",
        "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet'))"
      ],
      "metadata": {
        "id": "hOw3JzhwgWcO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "lifelog_data = {}\n",
        "\n",
        "# íŒŒì¼ë³„ë¡œ ì½ê¸°\n",
        "for file_path in parquet_files:\n",
        "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '')\n",
        "    lifelog_data[name] = pd.read_parquet(file_path)\n",
        "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hns0k9zvhGuY",
        "outputId": "df86a75d-56a1-4446-b36c-bf0fca8188a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded: mActivity, shape = (961062, 3)\n",
            "âœ… Loaded: mACStatus, shape = (939896, 3)\n",
            "âœ… Loaded: mAmbience, shape = (476577, 3)\n",
            "âœ… Loaded: mBle, shape = (21830, 3)\n",
            "âœ… Loaded: mLight, shape = (96258, 3)\n",
            "âœ… Loaded: mGps, shape = (800611, 3)\n",
            "âœ… Loaded: mUsageStats, shape = (45197, 3)\n",
            "âœ… Loaded: mScreenStatus, shape = (939653, 3)\n",
            "âœ… Loaded: wHr, shape = (382918, 3)\n",
            "âœ… Loaded: mWifi, shape = (76336, 3)\n",
            "âœ… Loaded: wLight, shape = (633741, 3)\n",
            "âœ… Loaded: wPedo, shape = (748100, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹\n",
        "for key, df in lifelog_data.items():\n",
        "    globals()[f\"{key}_df\"] = df"
      ],
      "metadata": {
        "id": "Z6o6qm8ob4xC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lifelog_dateê°€ timestampë‘ ê°™ë‹¤\n",
        "metrics_train = pd.read_csv('/content/drive/MyDrive/ETRI_lifelog_dataset/ch2025_metrics_train.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/ETRI_lifelog_dataset/ch2025_submission_sample.csv')"
      ],
      "metadata": {
        "id": "eo6L8gHpPT2n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date)\n",
        "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date'])\n",
        "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date))\n",
        "\n",
        "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì •\n",
        "dataframes = {\n",
        "    'mACStatus': (mACStatus_df, 'timestamp'),\n",
        "    'mActivity': (mActivity_df, 'timestamp'),\n",
        "    'mAmbience': (mAmbience_df, 'timestamp'),\n",
        "    'mBle': (mBle_df, 'timestamp'),\n",
        "    'mGps': (mGps_df, 'timestamp'),\n",
        "    'mLight': (mLight_df, 'timestamp'),\n",
        "    'mScreenStatus': (mScreenStatus_df, 'timestamp'),\n",
        "    'mUsageStats': (mUsageStats_df, 'timestamp'),\n",
        "    'mWifi': (mWifi_df, 'timestamp'),\n",
        "    'wHr': (wHr_df, 'timestamp'),\n",
        "    'wLight': (wLight_df, 'timestamp'),\n",
        "    'wPedo': (wPedo_df, 'timestamp'),\n",
        "}\n",
        "\n",
        "# âœ… ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'):\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
        "    df = df.dropna(subset=[timestamp_col])\n",
        "    df['date_only'] = df[timestamp_col].dt.date\n",
        "    df['key'] = list(zip(df[subject_col], df['date_only']))\n",
        "\n",
        "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])\n",
        "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key'])\n",
        "    return test_df, train_df\n",
        "\n",
        "# âœ… ê²°ê³¼ ì €ì¥\n",
        "for name, (df, ts_col) in dataframes.items():\n",
        "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\")\n",
        "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col)\n",
        "    globals()[f\"{name}_test\"] = test_df\n",
        "    globals()[f\"{name}_train\"] = train_df\n",
        "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MPug0C1VY2Z",
        "outputId": "b1c89b0d-2c89-43e0-f9f4-268b97ab3363"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ mACStatus ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mACStatus_test â†’ (335849, 3), mACStatus_train â†’ (604047, 3)\n",
            "â³ mActivity ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mActivity_test â†’ (343579, 3), mActivity_train â†’ (617483, 3)\n",
            "â³ mAmbience ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mAmbience_test â†’ (170453, 3), mAmbience_train â†’ (306124, 3)\n",
            "â³ mBle ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mBle_test â†’ (8140, 3), mBle_train â†’ (13690, 3)\n",
            "â³ mGps ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mGps_test â†’ (287386, 3), mGps_train â†’ (513225, 3)\n",
            "â³ mLight ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mLight_test â†’ (34439, 3), mLight_train â†’ (61819, 3)\n",
            "â³ mScreenStatus ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mScreenStatus_test â†’ (336160, 3), mScreenStatus_train â†’ (603493, 3)\n",
            "â³ mUsageStats ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mUsageStats_test â†’ (16499, 3), mUsageStats_train â†’ (28698, 3)\n",
            "â³ mWifi ë¶„ë¦¬ ì¤‘...\n",
            "âœ… mWifi_test â†’ (27467, 3), mWifi_train â†’ (48869, 3)\n",
            "â³ wHr ë¶„ë¦¬ ì¤‘...\n",
            "âœ… wHr_test â†’ (143311, 3), wHr_train â†’ (239607, 3)\n",
            "â³ wLight ë¶„ë¦¬ ì¤‘...\n",
            "âœ… wLight_test â†’ (233809, 3), wLight_train â†’ (399932, 3)\n",
            "â³ wPedo ë¶„ë¦¬ ì¤‘...\n",
            "âœ… wPedo_test â†’ (288832, 9), wPedo_train â†’ (459268, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë¶„ë‹¨ìœ„ -> í•˜ë£¨ ë‹¨ìœ„ def"
      ],
      "metadata": {
        "id": "ybuDtSKSuivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mACStatus(df):\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    df = df.sort_values(['subject_id', 'timestamp'])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        status = group['m_charging'].values  # 0/1 ìƒíƒœ\n",
        "        times = group['timestamp'].values\n",
        "\n",
        "        # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨\n",
        "        ratio_charging = status.mean()\n",
        "\n",
        "        # ìƒíƒœ ì „ì´ íšŸìˆ˜\n",
        "        transitions = (status[1:] != status[:-1]).sum()\n",
        "\n",
        "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤\n",
        "        lengths = []\n",
        "        current_len = 0\n",
        "        for val in status:\n",
        "            if val == 1:\n",
        "                current_len += 1\n",
        "            elif current_len > 0:\n",
        "                lengths.append(current_len)\n",
        "                current_len = 0\n",
        "        if current_len > 0:\n",
        "            lengths.append(current_len)\n",
        "\n",
        "        avg_charging_duration = np.mean(lengths) if lengths else 0\n",
        "        max_charging_duration = np.max(lengths) if lengths else 0\n",
        "\n",
        "        results.append({\n",
        "            'subject_id': subj,\n",
        "            'date': date,\n",
        "            'charging_ratio': ratio_charging,\n",
        "            'charging_transitions': transitions,\n",
        "            'avg_charging_duration': avg_charging_duration,\n",
        "            'max_charging_duration': max_charging_duration,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "mACStatus_df2 = process_mACStatus(mACStatus_df)"
      ],
      "metadata": {
        "id": "N3dswJou4PGl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mActivity(df):\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    summary = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        counts = group['m_activity'].value_counts(normalize=True)  # ë¹„ìœ¨\n",
        "        row = {'subject_id': subj, 'date': date}\n",
        "\n",
        "        # 0~8 ë¹„ìœ¨ ì €ì¥\n",
        "        for i in range(9):\n",
        "            row[f'activity_{i}_ratio'] = counts.get(i, 0)\n",
        "\n",
        "        # ì£¼ìš” í™œë™ ì •ë³´\n",
        "        row['dominant_activity'] = group['m_activity'].mode()[0]\n",
        "        row['num_unique_activities'] = group['m_activity'].nunique()\n",
        "\n",
        "        summary.append(row)\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "mActivity_df2 = process_mActivity(mActivity_df)"
      ],
      "metadata": {
        "id": "ahAmyl1S6937"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì§€ì •ëœ 10ê°œ ë¼ë²¨\n",
        "top_10_labels = [\n",
        "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\",\n",
        "    \"Narration, monologue\", \"Child speech, kid speaking\",\n",
        "    \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\"\n",
        "]\n",
        "\n",
        "def process_mAmbience_top10(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    # ì´ˆê¸°í™”\n",
        "    for label in top_10_labels + ['others']:\n",
        "        df[label] = 0.0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience']\n",
        "        others_prob = 0.0\n",
        "\n",
        "        for label, prob in parsed:\n",
        "            prob = float(prob)\n",
        "            if label in top_10_labels:\n",
        "                df.at[idx, label] = prob\n",
        "            else:\n",
        "                others_prob += prob\n",
        "\n",
        "        df.at[idx, 'others'] = others_prob\n",
        "\n",
        "    return df.drop(columns=['m_ambience'])\n",
        "\n",
        "mAmbience_df2= process_mAmbience_top10(mAmbience_df)\n",
        "\n",
        "def summarize_mAmbience_daily(df):\n",
        "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']]\n",
        "\n",
        "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½\n",
        "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index()\n",
        "    return daily_summary\n",
        "\n",
        "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2)"
      ],
      "metadata": {
        "id": "uMYc3WNwAyG0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mBle(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble']\n",
        "\n",
        "        rssi_list = []\n",
        "        class_0_cnt = 0\n",
        "        class_other_cnt = 0\n",
        "\n",
        "        for device in entry:\n",
        "            try:\n",
        "                rssi = int(device['rssi'])\n",
        "                rssi_list.append(rssi)\n",
        "\n",
        "                if str(device['device_class']) == '0':\n",
        "                    class_0_cnt += 1\n",
        "                else:\n",
        "                    class_other_cnt += 1\n",
        "            except:\n",
        "                continue  # malformed record\n",
        "\n",
        "        feature = {\n",
        "            'subject_id': row['subject_id'],\n",
        "            'date': row['date'],\n",
        "            'device_class_0_cnt': class_0_cnt,\n",
        "            'device_class_others_cnt': class_other_cnt,\n",
        "            'device_count': len(rssi_list),\n",
        "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan,\n",
        "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan,\n",
        "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan,\n",
        "        }\n",
        "        features.append(feature)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "def summarize_mBle_daily(df):\n",
        "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ\n",
        "    df = process_mBle(df)\n",
        "\n",
        "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸°\n",
        "    grouped = df.groupby(['subject_id', 'date']).agg({\n",
        "        'device_class_0_cnt': 'sum',\n",
        "        'device_class_others_cnt': 'sum',\n",
        "        'rssi_mean': 'mean',\n",
        "        'rssi_min': 'min',\n",
        "        'rssi_max': 'max',\n",
        "    }).reset_index()\n",
        "\n",
        "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚°\n",
        "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt']\n",
        "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan)\n",
        "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan)\n",
        "\n",
        "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±°\n",
        "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True)\n",
        "\n",
        "    return grouped\n",
        "\n",
        "mBle_df2 = summarize_mBle_daily(mBle_df)"
      ],
      "metadata": {
        "id": "-m7j1Yc1EDrP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mGps(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps']\n",
        "\n",
        "        altitudes = []\n",
        "        latitudes = []\n",
        "        longitudes = []\n",
        "        speeds = []\n",
        "\n",
        "        for entry in gps_list:\n",
        "            try:\n",
        "                altitudes.append(float(entry['altitude']))\n",
        "                latitudes.append(float(entry['latitude']))\n",
        "                longitudes.append(float(entry['longitude']))\n",
        "                speeds.append(float(entry['speed']))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        features.append({\n",
        "            'subject_id': row['subject_id'],\n",
        "            'date': row['date'],\n",
        "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan,\n",
        "            'latitude_std': np.std(latitudes) if latitudes else np.nan,\n",
        "            'longitude_std': np.std(longitudes) if longitudes else np.nan,\n",
        "            'speed_mean': np.mean(speeds) if speeds else np.nan,\n",
        "            'speed_max': np.max(speeds) if speeds else np.nan,\n",
        "            'speed_std': np.std(speeds) if speeds else np.nan,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "m_Gps_df2 = process_mGps(mGps_df)\n",
        "\n",
        "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({\n",
        "    'altitude_mean': 'mean',\n",
        "    'latitude_std': 'mean',\n",
        "    'longitude_std': 'mean',\n",
        "    'speed_mean': 'mean',\n",
        "    'speed_max': 'max',\n",
        "    'speed_std': 'mean'\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "oyL7ADbUNP_z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mLight(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "\n",
        "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„\n",
        "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6)\n",
        "\n",
        "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½\n",
        "    daily = df.groupby(['subject_id', 'date']).agg(\n",
        "        light_mean=('m_light', 'mean'),\n",
        "        light_std=('m_light', 'std'),\n",
        "        light_max=('m_light', 'max'),\n",
        "        light_min=('m_light', 'min'),\n",
        "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()),\n",
        "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()),\n",
        "        light_night_ratio=('is_night', 'mean')  # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨\n",
        "    ).reset_index()\n",
        "\n",
        "    return daily\n",
        "\n",
        "mLight_df2 = process_mLight(mLight_df)"
      ],
      "metadata": {
        "id": "-QXkKcr7ODUi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mScreenStatus(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        status = group['m_screen_use'].values\n",
        "        ratio_on = status.mean()\n",
        "        transitions = (status[1:] != status[:-1]).sum()\n",
        "\n",
        "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤\n",
        "        durations = []\n",
        "        current = 0\n",
        "        for val in status:\n",
        "            if val == 1:\n",
        "                current += 1\n",
        "            elif current > 0:\n",
        "                durations.append(current)\n",
        "                current = 0\n",
        "        if current > 0:\n",
        "            durations.append(current)\n",
        "\n",
        "        features.append({\n",
        "            'subject_id': subj,\n",
        "            'date': date,\n",
        "            'screen_on_ratio': ratio_on,\n",
        "            'screen_on_transitions': transitions,\n",
        "            'screen_on_duration_avg': np.mean(durations) if durations else 0,\n",
        "            'screen_on_duration_max': np.max(durations) if durations else 0,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df)"
      ],
      "metadata": {
        "id": "C72VGXW3O2Gt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_apps = [\n",
        "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', 'ì„±ê²½ì¼ë…Q',\n",
        "    'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram']\n",
        "\n",
        "def process_mUsageStats(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        app_time = {app: 0 for app in top_apps}\n",
        "        others_time = 0\n",
        "\n",
        "        for row in group['m_usage_stats']:\n",
        "            parsed = ast.literal_eval(row) if isinstance(row, str) else row\n",
        "            for entry in parsed:\n",
        "                app = entry.get('app_name')\n",
        "                time = entry.get('total_time', 0)\n",
        "                if app in top_apps:\n",
        "                    app_time[app] += int(time)\n",
        "                else:\n",
        "                    others_time += int(time)\n",
        "\n",
        "        feature = {\n",
        "            'subject_id': subj,\n",
        "            'date': date,\n",
        "            'others_time': others_time\n",
        "        }\n",
        "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€\n",
        "        feature.update({f'{app}_time': app_time[app] for app in top_apps})\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "mUsageStats_df2 = process_mUsageStats(mUsageStats_df)"
      ],
      "metadata": {
        "id": "bzzmA6ZYVqvR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_mWifi(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        rssi_all = []\n",
        "\n",
        "        for row in group['m_wifi']:\n",
        "            parsed = ast.literal_eval(row) if isinstance(row, str) else row\n",
        "            for ap in parsed:\n",
        "                try:\n",
        "                    rssi = int(ap['rssi'])\n",
        "                    rssi_all.append(rssi)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        results.append({\n",
        "            'subject_id': subj,\n",
        "            'date': date,\n",
        "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan,\n",
        "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan,\n",
        "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan,\n",
        "            'wifi_detected_cnt': len(rssi_all)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "mWifi_df2 = process_mWifi(mWifi_df)"
      ],
      "metadata": {
        "id": "WACOMvLUW1va"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_block(hour):\n",
        "    if 0 <= hour < 6:\n",
        "        return 'early_morning'\n",
        "    elif 6 <= hour < 12:\n",
        "        return 'morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'afternoon'\n",
        "    else:\n",
        "        return 'evening'\n",
        "\n",
        "def process_wHr_by_timeblock(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    df['block'] = df['timestamp'].dt.hour.map(get_time_block)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        block_stats = {'subject_id': subj, 'date': date}\n",
        "\n",
        "        for block, block_group in group.groupby('block'):\n",
        "            hr_all = []\n",
        "            for row in block_group['heart_rate']:\n",
        "                parsed = ast.literal_eval(row) if isinstance(row, str) else row\n",
        "                hr_all.extend([int(h) for h in parsed if h is not None])\n",
        "\n",
        "            if not hr_all:\n",
        "                continue\n",
        "\n",
        "            above_100 = [hr for hr in hr_all if hr > 100]\n",
        "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all)\n",
        "            block_stats[f'hr_{block}_std'] = np.std(hr_all)\n",
        "            block_stats[f'hr_{block}_max'] = np.max(hr_all)\n",
        "            block_stats[f'hr_{block}_min'] = np.min(hr_all)\n",
        "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all)\n",
        "\n",
        "        results.append(block_stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "wHr_df2 = process_wHr_by_timeblock(wHr_df)"
      ],
      "metadata": {
        "id": "7FBiJ4fnYLf2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_block(hour):\n",
        "    if 0 <= hour < 6:\n",
        "        return 'early_morning'\n",
        "    elif 6 <= hour < 12:\n",
        "        return 'morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'afternoon'\n",
        "    else:\n",
        "        return 'evening'\n",
        "\n",
        "def process_wLight_by_timeblock(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    df['block'] = df['timestamp'].dt.hour.map(get_time_block)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
        "        block_stats = {'subject_id': subj, 'date': date}\n",
        "\n",
        "        for block, block_group in group.groupby('block'):\n",
        "            lux = block_group['w_light'].dropna().values\n",
        "            if len(lux) == 0:\n",
        "                continue\n",
        "\n",
        "            block_stats[f'wlight_{block}_mean'] = np.mean(lux)\n",
        "            block_stats[f'wlight_{block}_std'] = np.std(lux)\n",
        "            block_stats[f'wlight_{block}_max'] = np.max(lux)\n",
        "            block_stats[f'wlight_{block}_min'] = np.min(lux)\n",
        "\n",
        "        results.append(block_stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
      ],
      "metadata": {
        "id": "Gr5ClTYbZEUt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_wPedo(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    summary = df.groupby(['subject_id', 'date']).agg({\n",
        "        'step': 'sum',\n",
        "        'step_frequency': 'mean',\n",
        "        'distance': 'sum',\n",
        "        'speed': ['mean', 'max'],\n",
        "        'burned_calories': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
        "    summary.columns = ['subject_id', 'date',\n",
        "                       'step_sum', 'step_frequency_mean',\n",
        "                       'distance_sum', 'speed_mean', 'speed_max',\n",
        "                       'burned_calories_sum']\n",
        "\n",
        "    return summary\n",
        "\n",
        "wPedo_df2 = process_wPedo(wPedo_df)"
      ],
      "metadata": {
        "id": "CPYQL1f4ZcAT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### df í•©ì¹˜ê¸°"
      ],
      "metadata": {
        "id": "6noR1u7GZ1cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "df_list = [\n",
        "    mACStatus_df2,\n",
        "    mActivity_df2,\n",
        "    mAmbience_df2,\n",
        "    mBle_df2,\n",
        "    m_Gps_df2,\n",
        "    mLight_df2,\n",
        "    mScreenStatus_df2,\n",
        "    mUsageStats_df2,\n",
        "    mWifi_df2,\n",
        "    wHr_df2,\n",
        "    wHr_df2,\n",
        "    wLight_df2,\n",
        "    wPedo_df2\n",
        "]\n",
        "\n",
        "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list)"
      ],
      "metadata": {
        "id": "mG4UCJMBZ5Up"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜\n",
        "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
        "\n",
        "# merged_dfì˜ dateë„ ë³€í™˜\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
        "\n",
        "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸°\n",
        "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'})\n",
        "\n",
        "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨\n",
        "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner')\n",
        "\n",
        "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date)\n",
        "merged_keys = merged_df[['subject_id', 'date']]\n",
        "train_keys = metrics_train_renamed[['subject_id', 'date']]\n",
        "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True)\n",
        "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left')"
      ],
      "metadata": {
        "id": "5Gb9JDXoZ9oM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ë§"
      ],
      "metadata": {
        "id": "zTPy9k120w6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸\n",
        "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
        "target_multiclass = 'S1'\n",
        "\n",
        "# âœ… feature ì¤€ë¹„\n",
        "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'])\n",
        "X.fillna(0, inplace=True)  # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
        "\n",
        "test_X = test_df.drop(columns=['subject_id', 'date'])\n",
        "test_X.fillna(0, inplace=True)\n",
        "\n",
        "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜\n",
        "def sanitize_column_names(df):\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "        .str.replace(r\"[^\\w]\", \"_\", regex=True)  # íŠ¹ìˆ˜ë¬¸ì â†’ _\n",
        "        .str.replace(r\"__+\", \"_\", regex=True)    # ì—°ì†ëœ _ ì œê±°\n",
        "        .str.strip(\"_\")                          # ì•ë’¤ _ ì œê±°\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# ëª¨ë“  ì…ë ¥ì— ì ìš©\n",
        "X = sanitize_column_names(X)\n",
        "test_X = sanitize_column_names(test_X)"
      ],
      "metadata": {
        "id": "NksL8xiz_3Oh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²°ê³¼ ì €ì¥\n",
        "binary_preds = {}\n",
        "multiclass_pred = None\n",
        "\n",
        "common_params = {\n",
        "    'n_estimators': 1000,\n",
        "    'learning_rate': 0.03,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "# ì´ì§„ ë¶„ë¥˜ í•™ìŠµ\n",
        "for col in targets_binary:\n",
        "    y = train_df[col]\n",
        "    model = LGBMClassifier(**common_params)\n",
        "    model.fit(X, y)\n",
        "    binary_preds[col] = model.predict(test_X)  # ğŸ”¥ í™•ë¥ X, í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡\n",
        "\n",
        "# ë‹¤ì¤‘ ë¶„ë¥˜ í•™ìŠµ (S1)\n",
        "y_multi = train_df['S1']\n",
        "model_s1 = LGBMClassifier(**common_params, objective='multiclass', num_class=3)\n",
        "model_s1.fit(X, y_multi)\n",
        "multiclass_pred = model_s1.predict(test_X)  # ğŸ”¥ í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡"
      ],
      "metadata": {
        "id": "oKpKyAngzkQ4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importance ì¶œë ¥\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model_s1.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 100))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "-J3OYiyHlXjV",
        "outputId": "e9624a0a-ec78-40e6-e56e-43468a030c5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41b51ec2a898>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# importance ì¶œë ¥\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m feature_importance = pd.DataFrame({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'importance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_s1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }).sort_values('importance', ascending=False)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
        "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
        "\n",
        "# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼\n",
        "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
        "\n",
        "# ID ë§Œë“¤ê¸° (submissionì—ì„œ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì—°ê²°í•˜ê¸° ìœ„í•´)\n",
        "submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬\n",
        "# ë³´í†µ ì˜ˆì¸¡ ê²°ê³¼ëŠ” test_df ê¸°ì¤€ì´ë¯€ë¡œ ì •ë ¬ ë³´ì¥ë˜ì–´ì•¼ í•¨\n",
        "assert len(submission_final) == len(multiclass_pred)  # shape ì²´í¬\n",
        "\n",
        "# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°\n",
        "submission_final['S1'] = multiclass_pred\n",
        "\n",
        "# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°\n",
        "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
        "    submission_final[col] = binary_preds[col].astype(int)  # í™•ë¥  ì•„ë‹Œ class ì˜ˆì¸¡\n",
        "\n",
        "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬\n",
        "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
        "\n",
        "# ì €ì¥\n",
        "submission_final.to_csv(\"submission_final.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"submission_final.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oQ7h8rOd1H6o",
        "outputId": "5fb39377-bc08-48bb-c500-682d5796c293"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_28e84404-1c69-48c7-b4ac-fa94340d2666\", \"submission_final.csv\", 9803)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}